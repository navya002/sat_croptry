{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout,UpSampling2D,merge\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "img_height=128\n",
    "img_width=128\n",
    "border=5\n",
    "path_train_input = 'D:/All/sat_crop/Ag-Net-Dataset/input/'\n",
    "path_train_output = 'D:/All/sat_crop/Ag-Net-Dataset/target/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_2015245_10_11\n"
     ]
    }
   ],
   "source": [
    "images_path = next(os.walk(path_train_input))[2]\n",
    "#print(images_path)\n",
    "images=np.asarray(images_path)\n",
    "#print(images)\n",
    "print(images[0][3:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:98: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1593, 128, 128, 7)\n"
     ]
    }
   ],
   "source": [
    "lst=[]\n",
    "j=1\n",
    "temp=[]\n",
    "\n",
    "for image in images:\n",
    "    i=(j%7)+1\n",
    "    if(j%7!=0):\n",
    "        img = load_img(path_train_input + 'lc8' + image[3:-6] + '_' + str(i) + '.tif', grayscale=True)\n",
    "        img = img_to_array(img)\n",
    "        img=np.array(img) \n",
    "        img=np.reshape(img, (16384,1))\n",
    "        #print(img.shape)\n",
    "        temp.append(img)\n",
    "        #lst.append(img)\n",
    "        j+=1\n",
    "    else:\n",
    "        img = load_img(path_train_input + 'lc8' + image[3:-6] + '_' + str(i) + '.tif', grayscale=True)\n",
    "        img = img_to_array(img)\n",
    "        img=np.array(img) \n",
    "        img=np.reshape(img, (16384,1))\n",
    "        #print(img.shape)\n",
    "        temp.append(img)\n",
    "        #lst.append(img)\n",
    "        j+=1\n",
    "        \n",
    "        temp=np.array(temp)\n",
    "        temp=np.reshape(temp,(128,128,7))\n",
    "        lst.append(temp)\n",
    "        temp=[]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "lst=np.array(lst)    \n",
    "print(lst.shape)\n",
    "#print(lst[0])\n",
    "#for v in img[0]:\n",
    " #   print((v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 7)\n"
     ]
    }
   ],
   "source": [
    "print((lst[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdl_2015245_10_12.tif\n",
      "_2015245_10_12\n",
      "(1594, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "images_path = next(os.walk(path_train_output))[2]\n",
    "#print(images_path)\n",
    "images=np.asarray(images_path)\n",
    "print(images[1])\n",
    "print(images[1][3:-4])\n",
    "lst=[]\n",
    "\n",
    "for image in images:\n",
    "    \n",
    "    \n",
    "    img = load_img(path_train_output + 'cdl' + image[3:-4]  + '.tif', grayscale=True)\n",
    "    img = img_to_array(img)\n",
    "    img=img/255\n",
    "    #img=np.array(img) \n",
    "    #img=np.reshape(img, (16384,1))\n",
    "    #print(img.shape)\n",
    "    lst.append(img)\n",
    "    #lst.append(img)\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "lst=np.array(lst)    \n",
    "print(lst.shape)\n",
    "#print(lst[0])\n",
    "#for v in img[0]:\n",
    " #   print((v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,x_test=x[:1274],x[1274:1433],x[1433:]\n",
    "y_train,y_val,y_test=y[:1274],y[1274:1433],y[1433:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.activations import softmax\n",
    "from keras.layers.core import Layer, Dense, Dropout, Activation, Flatten, Reshape, Permute\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the get_unet0 funcrtion is yet to be tried  : (below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_channels = 7\n",
    "num_mask_channels = 255\n",
    "\n",
    "\n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    return -K.log(jaccard_coef(y_true, y_pred)) + binary_crossentropy(y_pred, y_true)\n",
    "\n",
    "\n",
    "def get_unet0():\n",
    "    inputs = Input((num_channels, img_rows, img_cols))\n",
    "    conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(inputs)\n",
    "    conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "    conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "    conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(conv1)\n",
    "    conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "    conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(pool1)\n",
    "    conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "    conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "    conv2 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(conv2)\n",
    "    conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "    conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(pool2)\n",
    "    conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "    conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "    conv3 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(conv3)\n",
    "    conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "    conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(pool3)\n",
    "    conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "    conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "    conv4 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(conv4)\n",
    "    conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "    conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(512, 3, 3, border_mode='same', init='he_uniform')(pool4)\n",
    "    conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
    "    conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
    "    conv5 = Convolution2D(512, 3, 3, border_mode='same', init='he_uniform')(conv5)\n",
    "    conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
    "    conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(up6)\n",
    "    conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
    "    conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
    "    conv6 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(conv6)\n",
    "    conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
    "    conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(up7)\n",
    "    conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "    conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "    conv7 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(conv7)\n",
    "    conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "    conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(up8)\n",
    "    conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "    conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    "    conv8 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(conv8)\n",
    "    conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "    conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(up9)\n",
    "    conv9 = BatchNormalization(mode=0, axis=1)(conv9)\n",
    "    conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "    conv9 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(conv9)\n",
    "    crop9 = Cropping2D(cropping=((16, 16), (16, 16)))(conv9)\n",
    "    conv9 = BatchNormalization(mode=0, axis=1)(crop9)\n",
    "    conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "    conv10 = Convolution2D(num_mask_channels, 1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# startexecuting back from here: from u_net model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras import *\n",
    "def get_unet(inputs):\n",
    "    concat_axis =3\n",
    "    conv1=Convolution2D(32,3,3,activation=\"relu\",border_mode=\"same\")(inputs)\n",
    "    conv1=Convolution2D(32,3,3,activation=\"relu\",border_mode=\"same\")(conv1)\n",
    "    pool1=MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "    \n",
    "    conv2=Convolution2D(64,3,3,activation=\"relu\",border_mode=\"same\")(pool1)\n",
    "    conv2=Convolution2D(64,3,3,activation=\"relu\",border_mode=\"same\")(conv2)\n",
    "    pool2=MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "    \n",
    "    conv3=Convolution2D(128,3,3,activation=\"relu\",border_mode=\"same\")(pool2)\n",
    "    conv3=Convolution2D(128,3,3,activation=\"relu\",border_mode=\"same\")(conv3)\n",
    "    pool3=MaxPooling2D(pool_size=(2,2))(conv3)\n",
    "    \n",
    "    conv4=Convolution2D(256,3,3,activation=\"relu\",border_mode=\"same\")(pool3)\n",
    "    conv4=Convolution2D(256,3,3,activation=\"relu\",border_mode=\"same\")(conv4)\n",
    "    pool4=MaxPooling2D(pool_size=(2,2))(conv4)\n",
    "    \n",
    "    conv5=Convolution2D(512,3,3,activation=\"relu\",border_mode=\"same\")(pool4)\n",
    "    conv5=Convolution2D(512,3,3,activation=\"relu\",border_mode=\"same\")(conv5)\n",
    "    \n",
    "    up6=merge([UpSampling2D(size=(2,2))(conv5),conv4],mode=\"concat\",concat_axis=1)\n",
    "    conv6=Convolution2D(256,3,3,activation=\"relu\",border_mode=\"same\")(up6)\n",
    "    conv6=Convolution2D(256,3,3,activation=\"relu\",border_same=\"same\")(conv6)\n",
    "    \n",
    "    up7=merge([UpSampling2D(size=(2,2))(conv6),conv3],mode=\"concat\",concat_axis=1)\n",
    "    conv7=Convolution2D(128,3,3,activation=\"relu\",border_mode=\"same\")(up7)\n",
    "    conv7=Convolution2D(128,3,3,activation=\"relu\",border_mode=\"same\")(conv7)\n",
    "    \n",
    "    \n",
    "    up8=merge([UpSampling2D(size=(2,2))(conv7),conv2],mode=\"concat\",concat_axis=1)\n",
    "    conv8=Convolution2D(64,3,3,activation=\"relu\",border_mode=\"same\")(up8)\n",
    "    conv8=Convolution2D(64,3,3,activation=\"reli\",border_mode=\"same\")(conv8)\n",
    "    \n",
    "    up9=merge([UpSampling2D(size=(2,2))(conv8),conv1],mode=\"concat\",concat_axis=1)\n",
    "    conv9=Convolution2D(32,3,3,activation=\"relu\",border_mode=\"same\")(up9)\n",
    "    conv9=Convolution2D(32,3,3,activation=\"relu\",border_mode=\"same\")(conv9)\n",
    "    \n",
    "    conv10=Convolution2D(255,1,1,activation=\"sigmoid\")(conv9)\n",
    "    \n",
    "    \n",
    "    #outputs = core.Activation('softmax')(c9)\n",
    "    \n",
    "    #model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    #return model\n",
    "    \n",
    "    model=Model(input=inputs,output=conv10)\n",
    "    model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\",metrics=[jaccard_coef,jaccard_coef_int,\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-03-11 19:23:27.167289] Creating and compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_uniform\")`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", kernel_initializer=\"he_uniform\")`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:69: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:73: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_uniform\")`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:76: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_uniform\")`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:77: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_uniform\")`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:82: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:84: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_uniform\")`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:85: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_uniform\")`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:92: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_uniform\")`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:97: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_uniform\")`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:100: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_uniform\")`\n",
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:101: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.layers' has no attribute 'UPSampling2D'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1c1c3ba2bd5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[{}] Creating and compiling model...'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_unet0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[{}] Reading train...'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-1c1c3ba2bd5d>\u001b[0m in \u001b[0;36mget_unet0\u001b[1;34m()\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0mconv5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvanced_activations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mELU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mconv6\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUPSampling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nearest'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[0mup6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconv5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;31m#conv6 = Conv2DTranspose(256,( 3, 3), strides=(2, 2), padding='same') (conv5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.layers' has no attribute 'UPSampling2D'"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, Cropping2D\n",
    "from keras.layers import UpSampling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import keras\n",
    "import h5py\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import History\n",
    "import pandas as pd\n",
    "from keras.backend import binary_crossentropy\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import random\n",
    "import threading\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "img_rows = 112\n",
    "img_cols = 112\n",
    "\n",
    "\n",
    "smooth = 1e-12\n",
    "\n",
    "num_channels = 16\n",
    "num_mask_channels = 1\n",
    "\n",
    "\n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    return -K.log(jaccard_coef(y_true, y_pred)) + binary_crossentropy(y_pred, y_true)\n",
    "\n",
    "\n",
    "def get_unet0():\n",
    "    inputs = Input((num_channels, img_rows, img_cols))\n",
    "    conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(inputs)\n",
    "    conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "    conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "    conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(conv1)\n",
    "    conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "    conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(pool1)\n",
    "    conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "    conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "    conv2 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(conv2)\n",
    "    conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "    conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(pool2)\n",
    "    conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "    conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "    conv3 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(conv3)\n",
    "    conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "    conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(pool3)\n",
    "    conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "    conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "    conv4 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(conv4)\n",
    "    conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "    conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(512, 3, 3, border_mode='same', init='he_uniform')(pool4)\n",
    "    conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
    "    conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
    "    conv5 = Convolution2D(512, 3, 3, border_mode='same', init='he_uniform')(conv5)\n",
    "    conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
    "    conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
    "\n",
    "    conv6= keras.layers.UPSampling2D(size=(2, 2), data_format=None, interpolation='nearest')(conv5)\n",
    "    up6 = concatenate([conv5, conv4])\n",
    "    #conv6 = Conv2DTranspose(256,( 3, 3), strides=(2, 2), padding='same') (conv5)\n",
    "    #up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(up6)\n",
    "    conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
    "    conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
    "    conv6 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(conv6)\n",
    "    conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
    "    conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
    "\n",
    "    #conv7 = Conv2DTranspose(128,( 3, 3), strides=(2, 2), padding='same') (conv6)\n",
    "    conv6= keras.layers.UPSampling2D(size=(2,2))(conv6)\n",
    "    up7 = concatenate([conv6, conv3])\n",
    "    #up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(up7)\n",
    "    conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "    conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "    conv7 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(conv7)\n",
    "    conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "    conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "   \n",
    "\n",
    "    #conv8 = Conv2DT\n",
    "    ranspose(64,( 3, 3), strides=(2, 2), padding='same') (conv7)\n",
    "    conv8= UPSampling2D(size=(2,2))(conv7)\n",
    "    up8 = concatenate([conv5, conv4])\n",
    "    #up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(up8)\n",
    "    conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "    conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    "    conv8 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(conv8)\n",
    "    conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "    conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    " \n",
    "    conv9= UPSampling2D(size=(2,2))(conv8)\n",
    "    up9 = concatenate([conv8, conv1])\n",
    "    #up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(up9)\n",
    "    conv9 = BatchNormalization(mode=0, axis=1)(conv9)\n",
    "    conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "    conv9 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(conv9)\n",
    "    crop9 = Cropping2D(cropping=((16, 16), (16, 16)))(conv9)\n",
    "    conv9 = BatchNormalization(mode=0, axis=1)(crop9)\n",
    "    conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "    conv10 = Convolution2D(num_mask_channels, 1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def flip_axis(x, axis):\n",
    "    x = np.asarray(x).swapaxes(axis, 0)\n",
    "    x = x[::-1, ...]\n",
    "    x = x.swapaxes(0, axis)\n",
    "    return x\n",
    "\n",
    "\n",
    "def form_batch(X, y, batch_size):\n",
    "    X_batch = np.zeros((batch_size, num_channels, img_rows, img_cols))\n",
    "    y_batch = np.zeros((batch_size, num_mask_channels, img_rows, img_cols))\n",
    "    X_height = X.shape[2]\n",
    "    X_width = X.shape[3]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        random_width = random.randint(0, X_width - img_cols - 1)\n",
    "        random_height = random.randint(0, X_height - img_rows - 1)\n",
    "\n",
    "        random_image = random.randint(0, X.shape[0] - 1)\n",
    "\n",
    "        y_batch[i] = y[random_image, :, random_height: random_height + img_rows, random_width: random_width + img_cols]\n",
    "        X_batch[i] = np.array(X[random_image, :, random_height: random_height + img_rows, random_width: random_width + img_cols])\n",
    "    return X_batch, y_batch\n",
    "\n",
    "\n",
    "class threadsafe_iter:\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            return self.it.next()\n",
    "\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "\n",
    "@threadsafe_generator\n",
    "def batch_generator(X, y, batch_size, horizontal_flip=False, vertical_flip=False, swap_axis=False):\n",
    "    while True:\n",
    "        X_batch, y_batch = form_batch(X, y, batch_size)\n",
    "\n",
    "        for i in range(X_batch.shape[0]):\n",
    "            xb = X_batch[i]\n",
    "            yb = y_batch[i]\n",
    "\n",
    "            if horizontal_flip:\n",
    "                if np.random.random() < 0.5:\n",
    "                    xb = flip_axis(xb, 1)\n",
    "                    yb = flip_axis(yb, 1)\n",
    "\n",
    "            if vertical_flip:\n",
    "                if np.random.random() < 0.5:\n",
    "                    xb = flip_axis(xb, 2)\n",
    "                    yb = flip_axis(yb, 2)\n",
    "\n",
    "            if swap_axis:\n",
    "                if np.random.random() < 0.5:\n",
    "                    xb = xb.swapaxes(1, 2)\n",
    "                    yb = yb.swapaxes(1, 2)\n",
    "\n",
    "            X_batch[i] = xb\n",
    "            y_batch[i] = yb\n",
    "\n",
    "        yield X_batch, y_batch[:, :, 16:16 + img_rows - 32, 16:16 + img_cols - 32]\n",
    "\n",
    "\n",
    "def save_model(model, cross):\n",
    "    json_string = model.to_json()\n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    json_name = 'architecture_' + cross + '.json'\n",
    "    weight_name = 'model_weights_' + cross + '.h5'\n",
    "    open(os.path.join('cache', json_name), 'w').write(json_string)\n",
    "    model.save_weights(os.path.join('cache', weight_name), overwrite=True)\n",
    "\n",
    "\n",
    "def save_history(history, suffix):\n",
    "    filename = 'history/history_' + suffix + '.csv'\n",
    "    pd.DataFrame(history.history).to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def read_model(cross=''):\n",
    "    json_name = 'architecture_' + cross + '.json'\n",
    "    weight_name = 'model_weights_' + cross + '.h5'\n",
    "    model = model_from_json(open(os.path.join('../src/cache', json_name)).read())\n",
    "    model.load_weights(os.path.join('../src/cache', weight_name))\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path = '../data'\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    print('[{}] Creating and compiling model...'.format(str(datetime.datetime.now())))\n",
    "\n",
    "    model = get_unet0()\n",
    "\n",
    "    print('[{}] Reading train...'.format(str(datetime.datetime.now())))\n",
    "    f = h5py.File(os.path.join(data_path, 'train_16.h5'), 'r')\n",
    "\n",
    "    X_train = f['train']\n",
    "\n",
    "    y_train = np.array(f['train_mask'])[:, 1]\n",
    "    y_train = np.expand_dims(y_train, 1)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    train_ids = np.array(f['train_ids'])\n",
    "\n",
    "    batch_size = 128\n",
    "    nb_epoch = 50\n",
    "\n",
    "    history = History()\n",
    "    callbacks = [\n",
    "        history,\n",
    "    ]\n",
    "\n",
    "    suffix = 'structures_3_'\n",
    "    model.compile(optimizer=Nadam(lr=1e-3), loss=jaccard_coef_loss, metrics=['binary_crossentropy', jaccard_coef_int])\n",
    "    model.fit_generator(batch_generator(X_train, y_train, batch_size, horizontal_flip=True, vertical_flip=True, swap_axis=True),\n",
    "                        nb_epoch=nb_epoch,\n",
    "                        verbose=1,\n",
    "                        samples_per_epoch=batch_size * 400,\n",
    "                        callbacks=callbacks,\n",
    "                        nb_worker=8\n",
    "                        )\n",
    "\n",
    "    save_model(model, \"{batch}_{epoch}_{suffix}\".format(batch=batch_size, epoch=nb_epoch, suffix=suffix))\n",
    "    save_history(history, suffix)\n",
    "\n",
    "    suffix = 'structures_4_'\n",
    "    model.compile(optimizer=Nadam(lr=1e-4), loss=jaccard_coef_loss, metrics=['binary_crossentropy', jaccard_coef_int])\n",
    "    model.fit_generator(\n",
    "        batch_generator(X_train, y_train, batch_size, horizontal_flip=True, vertical_flip=True, swap_axis=True),\n",
    "        nb_epoch=nb_epoch,\n",
    "        verbose=1,\n",
    "        samples_per_epoch=batch_size * 400,\n",
    "        callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "    save_model(model, \"{batch}_{epoch}_{suffix}\".format(batch=batch_size, epoch=nb_epoch, suffix=suffix))\n",
    "    save_history(history, suffix)\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference if needed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def jaccard_coef(y_true, y_pred):\n",
    "    # __author__ = Vladimir Iglovikov\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    # __author__ = Vladimir Iglovikov\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs= Input((128,128,7))\n",
    "    \n",
    "\n",
    "model = get_unet(inputs)\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(X_train, y_train, batch_size=256, epochs=100, callbacks=callbacks,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"D:/All/sat_crop/Ag-Net-Dataset/model.json\", \"w\") as json_file: #Check the folder path once :NOTE\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"D:/All/sat_crop/Ag-Net-Dataset/model.h5\") #Check the folder path once :NOTE\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "json_file = open('D:/All/VipulSirProject/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"D:/All/VipulSirProject/model.h5\")\n",
    "print(\"Loaded model from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
